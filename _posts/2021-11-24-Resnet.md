---
title: Resnet - Deep Residual Learning for Image Recognition
tags:  ImageDetection ImageSegmentation Guides
---


The Background for proposing this Neural Network model, was the challenge of implementing deeper CNNs to achieve better classification performance. Though deeper CNNs resulted with improved performance, for various tasks such as recognition, detction, segmentation etc.On the other hand, for much deeper networks, problems such as vanishing/exploding gradients became more significant, so the extra layers add errors and accuracy degradation. Resnet proposes a model which enables training very deep networks - the [Resnet paper](https://arxiv.org/abs/1512.03385) - it presents Resnet-34, Resnet-50, Resnet-101 and Resnet-152, which deploy 34, 50, 101 and 152 parametric layers respectively.
Resnet building block is called a `Residual Block` which contains a `skip` connection, aka a `short-cut` as depicted in Figure 1 and described next.



***Figure 1: Resnet Residual Block***

![Resnet Residual Block](https://github.com/ronen-halevy/ronen-halevy.github.io/blob/master/assets/images/cnn-models/resnet-residual-block.drawio.png)

As shown by Figure 1, the skip is added to plain connection just before the Relu module, thus providing nolinearity effect to the sum.

The Resnet stacks Residual Blocks back to back as illustrated by Figure 2.


***Figure 1: Resnet - A Stack of Residual Blocks***

![Resnet Residual Block](https://github.com/ronen-halevy/ronen-halevy.github.io/blob/master/assets/images/cnn-models/resnet-stack-of-residual-block.drawio.png)

The skip connections pass information deeper into the network. An ituitive explaination for the performance improvement caused by the skip connections, suggests that if the weights  help with the vanishing/exploding gradient problem.


.  tasks such as by exploiting deepermodels.

# Resnet - Deep Residual Learning for Image Recognition

