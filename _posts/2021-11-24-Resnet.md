---
title: Resnet - Deep Residual Learning for Image Recognition
tags:  ImageDetection ImageSegmentation Guides
---


The Background for proposing this Neural Network model, was the challenge of implementing deeper CNNs to achieve better classification performance. Though deeper CNNs resulted with improved performance, for various tasks such as recognition, detction, segmentation etc.On the other hand, for much deeper networks, problems such as vanishing/exploding gradients became more significant, so the extra layers add errors and accuracy degradation. Resnet proposes a model which enables training very deep networks - the [Resnet paper](https://arxiv.org/abs/1512.03385) - it presents Resnet-34, Resnet-50, Resnet-101 and Resnet-152, which deploy 34, 50, 101 and 152 parametric layers respectively.
Resnet building block is called a `Residual Block` which contains a `skip` connection, aka a `short-cut` as depicted in Figure 1 and described next.



***Figure 1: Resnet Residual Block***

![Resnet Residual Block](https://github.com/ronen-halevy/ronen-halevy.github.io/blob/master/assets/images/cnn-models/resnet-residual-block.drawio.png)

As shown by Figure 1, the skip is added to plain connection just before the Relu module, thus providing nolinearity effect to the sum.

The Resnet stacks Residual Blocks back to back as illustrated by Figure 2.


***Figure 1: Resnet - A Stack of Residual Blocks***

![Resnet Residual Block](https://github.com/ronen-halevy/ronen-halevy.github.io/blob/master/assets/images/cnn-models/resnet-stack-of-residual-block.drawio.png)


***A brief reminder of vanishing gradient problem*** - during the backpropagation, the network's weights parameters are gradually updated to minimize the loss function, according to Gradient Descent algorithm - or any of its variants.

Here's the Gradient Descent update equation, for uptating the weights of the kth layer at time \\(i+1\\):

$w_{i+1}^k=w_i^k+\alpha * \frac{d L}{dw_i^k} $


Where:

- \\(w_{i+1}^k\\) expresses the kth layer's weights at time \\(i+1\\).
- \\(\alph\\) is the learning rate
- \frac{d L}{dw_i^k} is the Loss gradient with respect to the weight at time \\(i+1\\).


The gradients are calculated using the derivative chain rule, where the optimization calculation is executed in a back propogating manner, starting from layer k=L, back till k=1. 

Let's illustrate the back propogation on a residual block, i.e. the Loss derivative with respect to \\(a^{k}\\), given \\(\frac{dL}{da^{(k+2)}}\\). 

See Diagram below, which is followed by the xhin rule detivative expression.

![Resnet Residual Block](https://github.com/ronen-halevy/ronen-halevy.github.io/blob/master/assets/images/cnn-models/chain-rule-resnet-stack-of-residual-block.drawio.png)

According to chain rule:

\\(\frac{dL}{da^{(k)}} =  \frac{dL}{da^{(k+2)}}\frac{da^{(k+2)}}{da^{(k)}}\\)

Consider that:

\\(a^{(k+2)}} =  g(F(a^{(k)}) +  a^{(k)})


Where g(x) is the activation function, ReLu in this example. ReLu is linear for positive arguments and zero otherwise, so let's concentrate on the nonw zero case.

In that case, the derivative of the `skipped` component is 1, so this component protects against vanishing gradient problem.




