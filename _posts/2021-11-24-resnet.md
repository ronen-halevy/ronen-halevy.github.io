---
title: Resnet - Deep Residual Learning for Image Recognition
tags:  ImageDetection ImageSegmentation Guides
---

$\begin{table}[]
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
layer\_name               & stride & up-sample & input size & outoup size                  & kernel       & filters & Resnet-34 \\ \hline
conv1                     &        &           & 224 x 224  & 112x112                      & 7x7          & 64      &           \\ \hline
                          & 2      & -         & 112x112    & 56x56                        & max pool 3x3 & 64      &           \\ \cline{2-8} 
\multirow{-2}{*}{conv2.1} & 1      & x1        & 56x56      & 56x56                        & 3x3          & 64      &           \\ \hline
conv2.2                   & 1      & x1        & 56x56      & 56x56                        & 3x3          & 64      &           \\ \hline
conv3.1                   & 2      & x2        & 56x56      & {\color[HTML]{000000} 28x28} & 3x3          & 128     &           \\ \hline
conv3.2                   & 1      & x1        & 28x28      & 28x28                        &              &         &           \\ \hline
conv4.1                   &        &           &            &                              &              &         &           \\ \cline{1-4} \cline{6-8} 
conv4.2                   &        &           &            & \multirow{-2}{*}{kkkkkkkk}   &              &         &           \\ \hline
conv5.1                   &        &           &            &                              &              &         &           \\ \hline
conv5.2                   &        &           &            &                              &              &         &           \\ \hline
FC                        &        &           &            &                              &              &         &           \\ \hline
FLOPS                     &        &           &            &                              &              &         &           \\ \hline
                          &        &           &            &                              &              &         &           \\ \hline
\end{tabular}
\end{table}
Escape spec$

$ere$

\\(\begin{table}[]
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
layer\_name               & stride & up-sample & input size & outoup size                  & kernel       & filters & Resnet-34 \\ \hline
conv1                     &        &           & 224 x 224  & 112x112                      & 7x7          & 64      &           \\ \hline
                          & 2      & -         & 112x112    & 56x56                        & max pool 3x3 & 64      &           \\ \cline{2-8} 
\multirow{-2}{*}{conv2.1} & 1      & x1        & 56x56      & 56x56                        & 3x3          & 64      &           \\ \hline
conv2.2                   & 1      & x1        & 56x56      & 56x56                        & 3x3          & 64      &           \\ \hline
conv3.1                   & 2      & x2        & 56x56      & {\color[HTML]{000000} 28x28} & 3x3          & 128     &           \\ \hline
conv3.2                   & 1      & x1        & 28x28      & 28x28                        &              &         &           \\ \hline
conv4.1                   &        &           &            &                              &              &         &           \\ \cline{1-4} \cline{6-8} 
conv4.2                   &        &           &            & \multirow{-2}{*}{kkkkkkkk}   &              &         &           \\ \hline
conv5.1                   &        &           &            &                              &              &         &           \\ \hline
conv5.2                   &        &           &            &                              &              &         &           \\ \hline
FC                        &        &           &            &                              &              &         &           \\ \hline
FLOPS                     &        &           &            &                              &              &         &           \\ \hline
                          &        &           &            &                              &              &         &           \\ \hline
\end{tabular}
\end{table}
Escape spec\\)


## Introduction

The Background for proposing this Neural Network model, was the challenge of implementing deeper CNNs to achieve better classification performance. Deeper CNNs resulted with improved performance. This is valid for a various of computer vision tasks such as recognition, detction, segmentation etc. On the other hand, when getting much deeper, problems such as vanishing/exploding gradients become more significant, with symptoms such as growing errors and accuracy degradation. 

In their paper from 2015, ["Deep Residual Learning for Image Recognition"](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren & Jian Sun proposed a model which enables the training very deep networks - the Resnet. The paper demonstrates Resnet-34, Resnet-50, Resnet-101 and Resnet-152, which deploy 34, 50, 101 and 152 parametric layers respectively.

## Resnet Functional Description

Resnet building block is called a `Residual Block` which contains a `skip` connection, aka a `short-cut` as depicted in Figure 1 and described next.



***Figure 1: Resnet Residual Block***

![Resnet Residual Block]({{ site.baseurl }}/assets/images/cnn-models/resnet-residual-block.drawio-n.png)




As shown by Figure 1, the skip is added to plain connection just before the Relu module, thus providing nolinearity effect to the sum. Note that each conv element is followed by a back normalization module and a ReLu activation module.
Note - the values of the stride s and the down-scale parameter d are discussed later.

A Resnet network stacks Residual Blocks back to back, as illustrated by Figure 2. 

***Figure 2: Resnet-18***

![Resnet Residual Block]({{ site.baseurl }}/assets/images/cnn-models/resnet-18-block-diagram.drawio.png)


Figure 2 depicts a Resnet-18 CNN, named so for the 18 layers. The last layer is an FC (fully connected) layer, which receives the flatten array of data and outputs N recognition classes through a softmax module. Resnet was evaluatedwith ImageNet 2012 classification dataset that consists 1000 classes.


## Bottle-neck Resnet

Obviously, yhe deeper the resnet is, the more computation operations are needed - those are counted as FLOPS, where each FLOP is an add/multiply operation.
To cope with that, deeper networks, from Resnet50 and up, replace the Resnet Residual block by a Bottlenek Resnet block, as depicted by Figure 3.


***Figure 3: Bottlenek Resnet Block***

![Resnet Residual Block]({{ site.baseurl }}/assets/images/cnn-models/rdeeper-bottleneck-resnet-residual.drawio.png)

The bottleneck block consists of 3 conv blocks: A single conv 3 x 3, wrapped between 2 conv 1 x 1 blocks. Comparing to the basic Residual Block, this one uses a single conv 3 x 3 module. Note that the conv 3 x 3 requires 9 times more FLOPS than a conv 1 x 1. The conv 3 x 3 is indeed named the `bottleneck`. The front conv 1 x 1 module scales dimenssions down, to offload the bottleneck, while the conv 1 x 1 scales it up.





## Dimenssions Matching






## Notes on Vanishing Gradient problem

***A brief reminder of vanishing gradient problem*** - during the backpropagation, the network's weights parameters are gradually updated to minimize the loss function, according to Gradient Descent algorithm - or any of its variants.

Here's the Gradient Descent update equation, for uptating the weights of the kth layer at time \\(i+1\\):

$w_{i+1}^k=w_i^k+\alpha * \frac{d L}{dw_i^k} $


Where:

- \\(w_{i+1}^k\\) expresses the kth layer's weights at time \\(i+1\\).
- \\(\alph\\) is the learning rate
- \frac{d L}{dw_i^k} is the Loss gradient with respect to the weight at time \\(i+1\\).


The gradients are calculated using the derivative chain rule, where the optimization calculation is executed in a back propogating manner, starting from layer k=L, back till k=1. 

Let's illustrate the back propogation on a residual block, i.e. the Loss derivative with respect to \\(a^{k}\\), given \\(\frac{dL}{da^{(k+2)}}\\). 

See Diagram below, which is followed by the xhin rule detivative expression.

![Resnet Residual Block](https://github.com/ronen-halevy/ronen-halevy.github.io/blob/master/assets/images/cnn-models/chain-rule-resnet-stack-of-residual-block.drawio.png)

According to chain rule:

\\(\frac{dL}{da^{(k)}} =  \frac{dL}{da^{(k+2)}}\frac{da^{(k+2)}}{da^{(k)}}\\)

Consider that:

\\(a^{(k+2)}} =  g(F(a^{(k)}) +  a^{(k)})


Where g(x) is the activation function, ReLu in this example. ReLu is linear for positive arguments and zero otherwise, so let's concentrate on the nonw zero case.

In that case, the derivative of the `skipped` component is 1, so this component protects against vanishing gradient problem.

## Notes on Dimenssions Matching

If the dimenssions of the plain section increae, so it now differs from the skip input dimenssions, 2 approaches can be taken:
1. Add extra zero padding to the skip data.
2. Use projection, i.e. convolve with 1 x 1 kernel, just to expand dimensions.

Besides improving vanishing gradients issue, the `skip` connections carry lower level information from initial layers which correspnd to lower level features. 
Adding this information to the higher level more abstract information extracted by the layers which follow, contributes to better performance.
